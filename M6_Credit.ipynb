{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cc8190",
   "metadata": {
    "id": "a5cc8190"
   },
   "source": [
    "# Module 6: CFA\n",
    "# Use Case: Credit Risk - Identifying Bad Credit Risks\n",
    "# ===========================================\n",
    "\n",
    "In this example, we aim to predict bad consumer credits, and we develop a classification model for this purpose, driven by loan and debtor attributes. We would use this model to accept or reject a customerâ€™s business.\n",
    "\n",
    "Dataset: Credit risk https://datahub.io/machine-learning/credit-g\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f08271",
   "metadata": {
    "id": "35f08271"
   },
   "outputs": [],
   "source": [
    "# package for working with tabular data\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Package for charting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns #charts\n",
    "\n",
    "# package for timing runtime\n",
    "import time\n",
    "\n",
    "# package for navigating the operating system\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0xu8WyTTVNGd",
   "metadata": {
    "id": "0xu8WyTTVNGd",
    "tags": []
   },
   "source": [
    "# Utilities: Governance and Fairness Functions\n",
    "\n",
    "First we will declare and introduce the functions we will be using to implement the Governance Framework, and principles of Fairness in our process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JE0VW_oeVnah",
   "metadata": {
    "id": "JE0VW_oeVnah"
   },
   "source": [
    "## Governance Framework and Utility Function Overview\n",
    "\n",
    "A Governance framework ensures models in an organisation achieve all their key stakeholder requirements satisfactorily, and in a safe, verifiable way. In this section we introduce functions we will use lter in thje use-case to control and monitor model development process to ensure our aims are achieved.\n",
    "\n",
    "Manys steps in a Governance Framework are qualitative, requiring professionals to assess, specify, approve or reject stages in model development. However, quantitative tools can be a powerful utility, allowing professionals to control and monitor a process, and reach judgements about model design, stability, and efficacy. \n",
    "\n",
    "We discuss the 5 stages of model development and the utility functions that can be used to support the Governance Framework:\n",
    "\n",
    "#### Stage1: Business Analysis\n",
    "We first define our stakeholder KPIs, which should be systematically defined. We introduce example functions that go some way to representing stakeholder KPIs, with visualizations, statistical tests and checks where appropriate. \n",
    "\n",
    "#### Stage2: Data Process\n",
    "Exploratory data analysis goes some way to examining the quality and nature of the data, looking at distributions, correlations, imbalances in the data. We use some utility functions to support this.\n",
    "\n",
    "#### Stage3: Model Design and Development\n",
    "From a governance point of view, model design and development is more qualitative, and requires good practice, statitically and in terms of the code implementation. Good commenting is essential, sanity checking of input and return values is advised, and in Python clear parameter declaration and control of source code, and code versions is essential too. \n",
    "We also need to ensure that the outcomes of our model are fair to different population groups, as well as having a good precision to protect the business from loan losses. We will introduce functions to ensure fairness.\n",
    "\n",
    "#### Stage4: Model Deployment\n",
    "Model deployment involes multiple stages of testing and authorization. We propose a challenger model to conduct part of this process, which is also used in the monitoring and reporting stage also.\n",
    "\n",
    "#### Stage5: Monitoring/Reporting\n",
    "During live running of the models, monitoring of data drift is essential, and for additional safety a challenger model can be run in parallel to the live model, to ensure the live model is functioning well with respect to stakeholder KPIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wbY4WfepSk9J",
   "metadata": {
    "id": "wbY4WfepSk9J"
   },
   "source": [
    "## Stage1 Business Analysts Governance Utilities\n",
    "In Stage1, Business Analysis, for the Governance Framework we need to establish the risk level of the model - in this case high, including its impact on the organisation and technology, regulatory risk, and so on. As the model will be used to make credit decisions which will expose tyhe business to losses, regulatory risk and more, this is a high risk application requiring higher levels of Governance scrutiny. \n",
    "We also need to establish who the stakeholders are, and specify analytics to verify that we are attaining good performance on each stakeholder's key performance indicators (KPI). (There is also the communication of how our model has achieved these KPIs which we will address towards the end of the workflow).\n",
    "\n",
    "In this section we cover the utility functions that will be needed to monitor (and enforce) stakeholder KPIs.\n",
    "\n",
    "\n",
    "### Stakeholder KPIs\n",
    "\n",
    "#### Stakeholder KPIs: Customer\n",
    "We assume for this case study that the customer KPI is whether they achieve they achieve a positive credit decision, and whether this is fair (see below). It is also important to explain to customers why they did not achieve a positive credit decision too. This can be captured by examining the FP rate, using accuracy. We can address this using our Fairness functions, further below, and we can wrap up a check on accuracy in the function kpi_review_customer_business_compliance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6EeW3fZ39tFq",
   "metadata": {
    "id": "6EeW3fZ39tFq"
   },
   "source": [
    "#### Stakeholder: The Lending Business\n",
    "\n",
    "From a business point of view precision is the focus, where we want to avoid false negatives, ie lend to individuals that the default. This risk has to be weighed against the need to write loans of course. We can wrap this up in the function kpi_review_customer_business_compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FWSZilgXwi6x",
   "metadata": {
    "id": "FWSZilgXwi6x"
   },
   "source": [
    "#### Stakeholder KPIs: Compliance and Regulatory\n",
    "Regulatory KPIs include fairness (similar to a customer's KPI above), and also a reasonable level of precision, or risk from loan losses(simlar to the lenders KPIs). \n",
    "Fairness in this context is accuracy, the number of false negatives, of people erroneously refused credit. Ensuring accuracy is similar across different groups is essential to ensuring the model is fair to all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zKnVsOl08H9D",
   "metadata": {
    "id": "zKnVsOl08H9D"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Performance metrics...\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def kpi_review_customer_business_compliance(mdl: object,\n",
    "                                  X_test: pd.DataFrame,\n",
    "                                  y_test: pd.DataFrame,\n",
    "                                  y_hat: pd.DataFrame) -> (float, float):   \n",
    "    '''\n",
    "    Args:\n",
    "       mdl: sklearn classifier model object\n",
    "       X_test: X variables, columns are features, rows are instances\n",
    "       y_test: actual target variable {1,0}\n",
    "       y_hat: prediction of target\n",
    "       \n",
    "    Returns:\n",
    "       Accuracy, Precision\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print(f\"Accuracy train: {mdl.score(X_test,y_test):.4f}, cross-validation: \",\n",
    "      f\"{mdl.score(X_test,y_test):.4f}\")\n",
    "    print(f\"Precision train: {precision_score(y_test, y_hat, average=None)[1]:.4f}, cross-validation: \",\n",
    "      f\"{precision_score(y_test,y_hat, average=None)[0]:.4f}\")\n",
    "    \n",
    "    return mdl.score(X_test,y_test), precision_score(y_test,y_hat, average=None)[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ORuFUeGXwfcb",
   "metadata": {
    "id": "ORuFUeGXwfcb"
   },
   "source": [
    "\n",
    "#### Stakeholder KPIs: Analyst and technical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2rm-orUwNZ8W",
   "metadata": {
    "id": "2rm-orUwNZ8W"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Analyst KPI: ROC Curve, f1, precision and accuracy of y_hat from a classifier\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "def kpi_review_analyst(mdl: object,\n",
    "                       X: np.array, \n",
    "                       y: np.array,\n",
    "                       y_hat: np.array) -> (float, float, float):\n",
    "   \n",
    "  '''\n",
    "  Args:\n",
    "       mdl: sklearn classifier model object\n",
    "       X: X variables, columns are features, rows are instances\n",
    "       y: actual target variable {1,0}\n",
    "       y_hat: prediction of target\n",
    "       \n",
    "    Returns:\n",
    "       f1, prec, rec: F1 score, precision, recall.\n",
    "\n",
    "    '''\n",
    "\n",
    "  #sanity\n",
    "  if X.shape[0] != y_test.shape[0]:\n",
    "    raise TypeError('Bad parameter: X.shape[0] != y_test.shape[0]')\n",
    "  if y.shape[0] != y_hat.shape[0]:\n",
    "    raise TypeError('Bad parameter: y_test.shape[0] != y_test_hat.shape[0]')\n",
    "  if (y.dtype != y_hat.dtype):\n",
    "    raise TypeError('Bad parameter: y_test.dtypes != y_test_hat.dtypes')\n",
    "\n",
    "  # F1, precision, recall...  \n",
    "  prec = precision_score(y_true=y[:], y_pred=y_hat[:])\n",
    "  rec = recall_score(y_true=y[:], y_pred=y_hat[:])\n",
    "  f1 = f1_score(y_true=y[:], y_pred=y_hat[:])\n",
    "\n",
    "  print(prec)\n",
    "\n",
    "  # ROC Curve\n",
    "  metrics.plot_roc_curve(mdl, X, y) \n",
    "  fpr, tpr, thresholds = metrics.roc_curve(y, y_hat)\n",
    "  plt.title('Credit Decisions ROC Curve')\n",
    "  plt.show()\n",
    "\n",
    "  return f1, prec, rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SzlVEOCtH9cR",
   "metadata": {
    "id": "SzlVEOCtH9cR"
   },
   "source": [
    "## Stage 2 Data Process Governance Utilities\n",
    "\n",
    "Exploratory data analysis should be thorough enough to reveal sytrengthes and weaknesses of the data. We can examin distributions and correlations, but we should explicitly check for imbalances in the dataset, particulalry the y variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WnD3Vn0yRW9t",
   "metadata": {
    "id": "WnD3Vn0yRW9t"
   },
   "outputs": [],
   "source": [
    "# Check for imbalances, charts pie of imbalances in the y variable \n",
    "# wrapped as a func as we will use it a few times..\n",
    "def kpi_imbalanced_y_check(y: pd.Series) -> bool:  \n",
    "  '''\n",
    "    Args:\n",
    "        y: Dataframe of only the y variable\n",
    "       \n",
    "    Returns:\n",
    "       bool: True if imbalanced, False, if not imbalanced.\n",
    "\n",
    "    '''\n",
    "\n",
    "  print('Dataset Balanced?')\n",
    "  print(y.value_counts())\n",
    "\n",
    "  # Convert to df...\n",
    "  df_y = pd.DataFrame(y)\n",
    "  class_col = y.name\n",
    "  df_y.groupby(df_y[class_col]).size().plot(kind='pie', y=class_col, label = \"Type\",  autopct='%1.1f%%')\n",
    "\n",
    "  #Rule of thumb... highest frequency class < 70% of observations\n",
    "  imbalanced = False\n",
    "  perc_split = df_y.value_counts() / df_y.shape[0]\n",
    "  if np.max(perc_split) >= 0.7:\n",
    "    print('Imbalanced y variable!')\n",
    "    imbalanced = True\n",
    "  \n",
    "  return imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FwJLI-GCIVTC",
   "metadata": {
    "id": "FwJLI-GCIVTC"
   },
   "source": [
    "## Stage 3 Model Design and Development Governance Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5uxLdc8j5FUi",
   "metadata": {
    "id": "5uxLdc8j5FUi"
   },
   "source": [
    "### Stage3a: Fairness\n",
    "\n",
    "For classifiers such as this model, we are looking for a probability of default threshold that is acceptable to us as the lender. If a customer has a probability of default above this threshold, we would reject the application, below and we would accept the application. By adjusting this probability of default threshold value, the false positive / false negative outputs from the model change, and need to be appropriate for the lenders capital buffer, liquidity, and risk tolerance. \n",
    "At the same time as the precision of the model is appropriate, we also need to ensure the model is fair at that level of precision (note that the false positive and false negative rates will change at different threshold values).\n",
    "One exercise to ensure fair models is to appropriately select a threshold value such that the model has similar accuracy across (ie a false negative rate) between protected and priviledged classes, and is similarly accurate for all underlying population groups. It is normally seen that there is a trade-off between accuracy and fairness when using machine learning models to make policy decisions and fairness bias should be carefully eliminated.\n",
    "\n",
    "These utility functions will be used to monitor fairness across different classes in the dataset, to ensure a similar level of accuracy in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r68W8AgUKXV9",
   "metadata": {
    "id": "r68W8AgUKXV9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def x_metrics(true_label, predicted_label):\n",
    "    #This module computes the confusion matrix columns\n",
    "    TN, FP, FN, TP = confusion_matrix(true_label, predicted_label, labels = [0,1]).ravel()\n",
    "    return TN, FP, FN, TP\n",
    "\n",
    "def trainModel(X_train, y_train):\n",
    "    rf1 = RandomForestClassifier(max_depth=5,random_state=0)\n",
    "    rf1.fit(X_train,y_train)\n",
    "    return rf1\n",
    "\n",
    "def predictModel(model, X_test, threshold=0.5):\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    return (y_pred_proba[:,0] < threshold).astype('int')\n",
    "\n",
    "def getFairnessRatios(y_test, y_pred):\n",
    "    TN, FP, FN, TP = x_metrics(y_test, y_pred)\n",
    "    accuracy = (TP + TN)/(TP+TN+FP+FN)\n",
    "    fnr = FN/(FN+TP)\n",
    "    fdr = FP/(FP+TP)\n",
    "    fpr = FP/(FP+TN)\n",
    "    npv = TN/(TN+FN)\n",
    "    recall = TP/(TP+FN)\n",
    "    precision = TP/(TP+FP)\n",
    "    return pd.Series({'Stats': '', 'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'fnr': fnr, 'fdr': fdr, 'fpr': fpr, 'npv': npv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X2QLrTgbKSsS",
   "metadata": {
    "id": "X2QLrTgbKSsS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tdh7AArePx7s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tdh7AArePx7s",
    "outputId": "feec880e-f045-41ef-fece-d77dbb60509f"
   },
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\"\"\"\n",
    "#Split first to avoid data-snooping\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "#****************************\n",
    "X_test_gender = X_test['gender']\n",
    "# Remember to remove protected columns before training\n",
    "X_train = X_train.drop(protected_cols, axis=1)\n",
    "X_test = X_test.drop(protected_cols, axis=1)\n",
    "#****************************\n",
    "\n",
    "df_stats = fairness_stats_get(max_mdl, X_test=X_test, y_test=y_test, X_test_category_col=X_test_gender)\n",
    "\"\"\"\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRy2N7-eRaJP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "TRy2N7-eRaJP",
    "outputId": "909f0900-ad14-4cf2-9e2f-00eecc2b9fad"
   },
   "outputs": [],
   "source": [
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Added by Madhu\n",
    "#def fairness_test(df_stats: pd.DataFrame, \n",
    "#                 compare_metrics: str = ['precision'], \n",
    "#                 critical_fairness_threshold: float = 0.2,\n",
    "#                 y_approval_threshold: float = 0.5):\n",
    "\n",
    "compare_metrics = ['precision'], \n",
    "critical_fairness_threshold = 0.2,\n",
    "y_approval_threshold = 0.5\n",
    "\n",
    "fig, axs = plt.subplots(5)\n",
    "\n",
    "\n",
    "\n",
    "#fairness_test(df_stats=df_stats)\n",
    "\n",
    "\"\"\"\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f272a9-f91c-4a2d-ab30-741fa05385b1",
   "metadata": {},
   "source": [
    "#Added by Madhu\n",
    "\n",
    "The process to identify an appropriate model in this context - that is also fair to the population groups, would be to review recall values that are within acceptable limits for different population groups, concurrently maximizing the overall model precision.  Multiple models can be configured by changing the model hyper parameters.  Iterating over the various models would help identify the optimal model.The steps are as below:\n",
    "\n",
    "* Generate a set of training data, which does not include any of the protected variables such as age/gender etc. This data would be used to train the model.\n",
    "* Generate a set of test data, with the protected variable data available as columns in the dataset.  The fairness testing modules will split the test data across each of the population group based on these columns, so the model predictions can be validated for each group.\n",
    "* Identify a list of appropriate modeling techniques, that help classify the loan application to be approved/rejected \n",
    "\n",
    "The following steps help identify the appropriate model:\n",
    "* For each model, apply multiple values of the threshold at which the classification decisions are made. The threshold at which an individual outcome is classified to be approved or not can make all the difference.\n",
    "* For each of the threshold value, fit the model using a specific set of hyper parameters\n",
    "* Using the model, predict outcomes for the test data.  The model is run once for each population group. For example, if the fairness test is required for gender, the test data will be in two parts - one for male applicants and the other for female applicants.  \n",
    "* Ensure that the recall values are within acceptable limits for each of the population groups\n",
    "* Review the model precision and retain the ones with higher precision\n",
    "\n",
    "Over a set of multiple model methodologies and hyper parameters, the optimum model that maximizes precision while being fair to the population groups can be identified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c332dc-902b-4307-a796-608d97f3a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added by Madhu\n",
    "def get_optimal_threshold (mod: object, \n",
    "                      X_test: pd.DataFrame, \n",
    "                      y_test: pd.DataFrame, \n",
    "                      X_test_category_col: pd.DataFrame,\n",
    "                      majority_class: str = \"Male\",\n",
    "                      fairness_metric: str = \"recall\",\n",
    "                      threshold_metric: str = \"precision\"):   \n",
    "   \n",
    "    '''    \n",
    "    Args:\n",
    "        df_stats: record of the accuracy (etc) of the model on each category.\n",
    "        majority_class: string - the value of the majority class against which the other population groups are compared with (e.g. [\"Male\"])\n",
    "        fairness_metric: array with the fairness metrics to compare e.g. [\"recall\"]\n",
    "        \n",
    "    Returns:\n",
    "        A plot charting the fairness metric values to the various population groups...\n",
    "    \n",
    "    Author:\n",
    "      Madhu Nagarajan\n",
    "\n",
    "    fairness_metric = 'recall'\n",
    "    threshold_metric = \"precision\"\n",
    "    majority_class = \"Male\"\n",
    "    '''\n",
    "    high_maximization_metric = 0\n",
    "    \n",
    "    #Try with multiple threshold values from 0.5 to 1.0.\n",
    "    for a_threshold in range (50, 100, 5):\n",
    "        fair_model = True\n",
    "        \n",
    "        #get the model metrics for a speicific threshold values\n",
    "        df_stats = fairness_stats_get (max_mod, X_test, y_test, X_test_category_col, a_threshold/100)\n",
    "        \n",
    "        #get the metric to compare for the majority class (e.g. Male)\n",
    "        majority_class_metric  = df_stats.loc[df_stats[\"cat\"] == majority_class, fairness_metric].astype('float64')[0]\n",
    "        \n",
    "        #Iterate through the various values for the selected group\n",
    "        for x in df_stats['cat'].values:\n",
    "    \n",
    "            if x not in [\"All\", majority_class]:\n",
    "                #ignore the category values of All and the majority class. obtain the fairness metric for the other population groups\n",
    "                compare_metric = df_stats.loc[df_stats[\"cat\"]==x][fairness_metric].astype('float64')[0]\n",
    "                \n",
    "                #Ensure the metric for all non majority classes are within limits, one sided ensures that the non majority classes are not worse off\n",
    "                if (majority_class_metric * 0.8 > compare_metric):  \n",
    "                    \n",
    "                    #if any metric is below limit, then set the model as not fair\n",
    "                    fair_model = 'False'\n",
    "                    #and try the next threshold\n",
    "                    \n",
    "        #\n",
    "        current_maximization_metric = df_stats.loc[df_stats[\"cat\"]==\"All\"][threshold_metric].astype('float64')[0]\n",
    "        \n",
    "        #if the model is found fair for all population groups (other than the majority one), then check if the model has a higher maximization metric. if so save the threshold value\n",
    "        if fair_model:\n",
    "            if current_maximization_metric > high_maximization_metric:\n",
    "                high_maximization_metric = current_maximization_metric\n",
    "                high_threshold = a_threshold\n",
    "            \n",
    "    if high_maximization_metric > 0:\n",
    "        df_stats = fairness_stats_get (max_mod, X_test, y_test, X_test_gender, high_threshold/100)\n",
    "        plot_fairness_charts(df_stats, majority_class, [fairness_metric, threshold_metric])\n",
    "        return high_threshold/100\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CKA0l02DWtyo",
   "metadata": {
    "id": "CKA0l02DWtyo"
   },
   "source": [
    "## Stage 4 Model Deployment Governance Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GRd3jH02K_gN",
   "metadata": {
    "id": "GRd3jH02K_gN"
   },
   "source": [
    "### Stage4a Challenger Models\n",
    "\n",
    "The following function, trains a challenger model using sklearn classifiers, and can be used to challenge our live model by comparing KPIs between our live  and challenger models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wpRpI2MQK_zr",
   "metadata": {
    "id": "wpRpI2MQK_zr"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Create a challenger model\n",
    "# requires a list of ready trained models (all_mdls) and corresponding descrtiptions \n",
    "# (all_mdls_desc) and performance (all_mdls_prec). Ensembled the better performers\n",
    "# and uses the resulting model as a challenger \n",
    "def challenger_ensemble_run(all_mdls: list, \n",
    "                  all_mdls_desc: list,\n",
    "                  all_mdls_prec: list,\n",
    "                  X_train: pd.DataFrame,\n",
    "                  y_train: pd.DataFrame, \n",
    "                  X_test: pd.DataFrame) -> (np.array, object):\n",
    "  '''\n",
    "  Args:\n",
    "    all_mdls: a list of sklearn classifiers, trained and ready to go.\n",
    "    all_mdls_desc: list of description of the above model objects (elements corresponding to the above also)       \n",
    "    all_mdls_prec: list of out-of-sample prec scores for each of the above model objects, used to elmininate the poor performers from the challenger (elements corresponding to the above also)       \n",
    "    X_train: DataFrame with training data for classifier, columns are features, rows are instances\n",
    "    X_test: Test data matching above shape\n",
    "    y_train: training data target variable {1,0}, instances are rows.\n",
    "    y_test: test data target variable {1,0}, instances are rows.\n",
    "      \n",
    "  Returns:\n",
    "      y_hat: numpy array containing predictions from the challenger\n",
    "      object: sklearn model object containing the challenger model.\n",
    "      \n",
    "  Author:\n",
    "      Dan Philps\n",
    "  '''\n",
    "\n",
    "  #Sanity\n",
    "  if X_train.shape[0] != y_train.shape[0]:\n",
    "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
    "  if X_test.shape[1] != X_train.shape[1]:\n",
    "    raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
    "\n",
    "  # Only use classifiers that have generated an above median F1 score out of sample.\n",
    "  min_prec_to_use = np.median(all_mdls_prec)\n",
    "  \n",
    "  # Prepare our models for ensembling\n",
    "  challenger_models = []\n",
    "  for i in range(0, all_mdls_desc.__len__()):\n",
    "    if all_mdls_prec[i] > min_prec_to_use:\n",
    "      challenger_models.append((all_mdls_desc[i], all_mdls[i]))\n",
    "\n",
    "  # Instantiate ...\n",
    "  vc = VotingClassifier(estimators=challenger_models, voting='soft')\n",
    "\n",
    "  # Fit on the training data to train your live model... \n",
    "  challenger_mdl = vc.fit(X_train.values, y_train.values)\n",
    "\n",
    "  # Challenge! Compare the results\n",
    "  y_hat = challenger_mdl.predict(X_test)\n",
    "  \n",
    "  return y_hat, challenger_mdl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yWZnZyV0-nX0",
   "metadata": {
    "id": "yWZnZyV0-nX0"
   },
   "source": [
    "When we finally produce our live model that will be driving the credit decision proces, we need a way of checking the live model's performance against the challenger's so we can trigger warnings if there is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ukqm9_KI-nrD",
   "metadata": {
    "id": "ukqm9_KI-nrD"
   },
   "outputs": [],
   "source": [
    "# Check to run on Challenger and live models that encapsulates key KPIs\n",
    "def kpi_review_challenger(live_mod: object,\n",
    "                          challenger_mod: object,\n",
    "                          X_test: pd.DataFrame,\n",
    "                          y_test: pd.DataFrame,\n",
    "                          precision_fault_threshold: float = 0.1,\n",
    "                          accuracy_fault_threshold: float = 0.1) -> str:\n",
    "  '''\n",
    "  Args:\n",
    "    live_mod: live model, trained and ready to go.\n",
    "    challenger_mod: challenger model to compare to live, trained and ready to go.\n",
    "    X_test: Test data matching above shape\n",
    "    y_test: test data target variable {1,0}, instances are rows.\n",
    "    precision_fault_threshold: if challenger is > this must better than live, throw an exception\n",
    "    accuracy_fault_threshold: if challenger is > this must better than live, throw an exception\n",
    "      \n",
    "  Returns:\n",
    "      err: error message\n",
    "      \n",
    "  Author:\n",
    "      Dan Philps\n",
    "  '''\n",
    "\n",
    "  # Run models and compare\n",
    "  y_hat_live = live_mod.predict(X_test)\n",
    "  y_hat_challenger = challenger_mod.predict(X_test)\n",
    "\n",
    "  # Compare the precsision of live and challenger\n",
    "  live_ac, live_prec = kpi_review_customer_business_compliance(live_mod, X_test, y_test, y_hat_live)\n",
    "  challenger_ac, challenger_prec = kpi_review_customer_business_compliance(challenger_mod, X_test, y_test, y_hat_challenger)\n",
    "\n",
    "  # Simple test\n",
    "  if (challenger_prec - live_prec > precision_fault_threshold):\n",
    "      err = 'Precision fault threshold breached! Challenger model achieving materially better precision than live - consider retraining live models.'\n",
    "      print('Precision fault threshold breached! Challenger model achieving materially better precision than live - consider retraining live models.')\n",
    "\n",
    "  if (live_prec - live_ac > accuracy_fault_threshold):\n",
    "      err = 'Precision fault threshold breached! Challenger model achieving materially better precision than live - consider retraining live models.'\n",
    "      print('Accuracy fault threshold breached! Challenger model achieving materially better accuracy than live - consider retraining live models.')\n",
    "  \n",
    "  # Bar chart of prec and recall\n",
    "  plt.bar(['live_prec', 'challenger_prec'], [live_prec, challenger_prec], color = 'b')\n",
    "  plt.bar(['live_ac', 'challenger_ac'], [live_ac, challenger_ac], color = 'r')\n",
    "  plt.title=('Bar chart of Precision and Accuracy')\n",
    "  plt.show()\n",
    "\n",
    "  # ROC Curve\n",
    "  y_hat_prob_live = live_mod.predict_proba(X_test)[:, 1]\n",
    "  y_hat_prob_challenger = challenger_mod.predict_proba(X_test)[:, 1]\n",
    "\n",
    "  plt.title = \"Credit Decisions ROC Curve\"\n",
    "  \n",
    "  fpr, tpr, _ = metrics.roc_curve(y_test, y_hat_prob_live)\n",
    "  plt.plot(fpr,tpr,label='Live model')\n",
    "\n",
    "  fpr, tpr, _ = metrics.roc_curve(y_test, y_hat_prob_challenger)\n",
    "  plt.plot(fpr,tpr,label='Challenger model')\n",
    "\n",
    "  plt.legend(['Live', 'Challenger'])\n",
    "  plt.title('ROC Curves: Live vs Challenger')\n",
    "  plt.show()\n",
    "\n",
    "  return err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1O4k1SyqEt9_",
   "metadata": {
    "id": "1O4k1SyqEt9_"
   },
   "source": [
    "## Stage5 Model Monitoring and Reporting Governance Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m_Po5KWc7eU3",
   "metadata": {
    "id": "m_Po5KWc7eU3"
   },
   "source": [
    "### Stage5a: Data drift monitoring\n",
    "After our model is released into the live environment, we need to ensure that the input data is not significantly different to tghe data the model was trained on. If it is the model outcomes could become unstable. We can use population stability index (PSI) to compare input data with training data, and we can also use a KS test to do the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tRJUaGgnEq8X",
   "metadata": {
    "id": "tRJUaGgnEq8X"
   },
   "outputs": [],
   "source": [
    "# Population Stability Index (PSI) can be applied to the input features or variables, also known as Characteristic \n",
    "# Stability Index (CSI), as well as the output of a scoring model, a model whose score may indicate the probability \n",
    "# of fraud, or probability of default. PSI captures the shift in the population distribution of values. If the score\n",
    "# distribution has shifted, one should then look to see what feature(s) or variable(s) is causing the shift.\n",
    "# A shift in the distribution of input features or features, or output score distribution could imply that the model \n",
    "# may need retrained. The common interpretation of PSI, which comes from the orignal work on credit models, is as follows: \n",
    "# PSI < 0.1: no significant population change, PSI < 0.2: moderate population change and PSI >= 0.2: significant population\n",
    "# change.\n",
    "\n",
    "#https://www.quora.com/What-is-population-stability-index\n",
    "\n",
    "def data_drift_psi(X_train: np.array,                  \n",
    "                  X: np.array, \n",
    "                  buckettype: str='bins', \n",
    "                  buckets: int =10, \n",
    "                  axis: int =0) -> np.ndarray:\n",
    "    '''Calculate the PSI (population stability index) across all variables\n",
    "    Args:\n",
    "       expected: numpy matrix of original values - both features and target\n",
    "       actual: numpy matrix of new values, same size as expected\n",
    "       buckettype: type of strategy for creating buckets, bins splits into even splits, \n",
    "       quantiles splits into quantile buckets\n",
    "       buckets: number of quantiles to use in bucketing variables\n",
    "       axis: axis by which variables are defined, 0 for vertical, 1 for horizontal \n",
    "       \n",
    "    Returns:\n",
    "       psi_values: ndarray of psi values for each variable\n",
    "       \n",
    "    Author:\n",
    "       Matthew Burke\n",
    "       github.com/mwburke\n",
    "       worksofchart.com\n",
    "    '''\n",
    "\n",
    "    # sanity\n",
    "    if X_train.shape[1] != X.shape[1]:\n",
    "      raise TypeError('X_train.shape[1] != X.shape[1]')\n",
    "\n",
    "    def psi(expected_array: np.array,        \n",
    "            actual_array: np.array,        \n",
    "            buckets: int) -> np.ndarray:\n",
    "        '''Calculate the PSI for a single variable \n",
    "        Args:\n",
    "           expected_array: numpy array of original values\n",
    "           actual_array: numpy array of new values, same size as expected\n",
    "           buckets: number of percentile ranges to bucket the values into\n",
    "           \n",
    "        Returns:\n",
    "           psi_value: calculated PSI value\n",
    "        \n",
    "        Author: Augustine Backer\n",
    "        '''\n",
    "        \n",
    "        def scale_range (input,\n",
    "                         min, \n",
    "                         max):\n",
    "            input += -(np.min(input))\n",
    "            input /= np.max(input) / (max - min)\n",
    "            input += min\n",
    "            return input\n",
    "\n",
    "        breakpoints = np.arange(0, buckets + 1) / (buckets) * 100\n",
    "        \n",
    "        if buckettype == 'bins':\n",
    "            breakpoints = scale_range(breakpoints, np.min(expected_array), np.max(expected_array))\n",
    "        elif buckettype == 'quantiles':\n",
    "            breakpoints = np.stack([np.percentile(expected_array, b) for b in breakpoints])\n",
    "    \n",
    "        expected_percents = np.histogram(expected_array, breakpoints)[0] / len(expected_array)\n",
    "        actual_percents = np.histogram(actual_array, breakpoints)[0] / len(actual_array)\n",
    "    \n",
    "\n",
    "        def sub_psi(e_perc: float, \n",
    "                    a_perc: float) -> float:\n",
    "            '''Calculate the actual PSI value from comparing the values.\n",
    "               Update the actual value to a very small number if equal to zero\n",
    "            '''\n",
    "            if a_perc == 0:\n",
    "                a_perc = 0.001\n",
    "\n",
    "                a_perc = 0.0001\n",
    "            if e_perc == 0:\n",
    "                e_perc = 0.0001\n",
    "\n",
    "            value = (e_perc - a_perc) * np.log(e_perc / a_perc)\n",
    "            return(value)\n",
    "        \n",
    "        psi_value = sum(sub_psi(expected_percents[i], actual_percents[i]) for i in range(0, len(expected_percents)))\n",
    "\n",
    "        return(psi_value)\n",
    "    \n",
    "    # The shape function returns the dimension of the array, with 1 being one variable being examined\n",
    "    # psi_values - a psi value will be calculated for each variable in the array\n",
    "    if len(X_train.shape) == 1:\n",
    "        psi_values = np.empty(len(X_train.shape))\n",
    "    else:\n",
    "        psi_values = np.empty(X_train.shape[axis])\n",
    "    \n",
    "    for i in range(0, len(psi_values)):\n",
    "        if len(psi_values) == 1:\n",
    "            psi_values = psi(X_train, X, buckets)\n",
    "        elif axis == 0:\n",
    "            psi_values[i] = psi(X_train[:,i], X[:,i], buckets)\n",
    "        elif axis == 1:\n",
    "            psi_values[i] = psi(X_train[i,:], X[i,:], buckets)\n",
    "    \n",
    "    return(psi_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "djgWAvcvHdpR",
   "metadata": {
    "id": "djgWAvcvHdpR"
   },
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov (KS) is used to measure the performance of classification models. More accurately, KS is a measure of \n",
    "# the degree of separation between the positive and negative distributions, for example deafult vs. non-default. K-S ranges\n",
    "# from 0% to 100%, and the higher the KS value is, the better the model is at separating the positive and negative \n",
    "# distributions. The KS statistic for two samples is simply the greatest distance between their two cummulative \n",
    "# distribution functions, so if we measure the distance between the positive and negative class distributions. \n",
    "# A KS of 0.6 or higher is considered good, associated with a low p-value.\n",
    "# https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test https://towardsdatascience.com/evaluating-classification-models-with-kolmogorov-smirnov-ks-test-e211025f5573\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def ks_test_result(model_score_target: pd.DataFrame,\n",
    "                   score, \n",
    "                   target) -> float:\n",
    "    '''\n",
    "    Args:\n",
    "        model_score_target: dataframe of the model output and binary target (1=event, 0=non event), with the colunns \n",
    "        labeled to match the target_column and score_column strings passed in\n",
    "        target: string of target column in the dataframe, example \"y\"\n",
    "        score: string of the score, or model output, typically [0,100], where the higher the score the higher\n",
    "        the likelihood of an event, for example \"score\"\n",
    "       \n",
    "    Returns:\n",
    "       ks_statistic: the KS value for the cummulative distribution function of the scores of events and non-events\n",
    "    \n",
    "    Author:\n",
    "      Augustine Backer\n",
    "    '''\n",
    "    ks=ks_2samp(model_score_target.loc[model_score_target[target]==0,score], \n",
    "                model_score_target.loc[model_score_target[target]==1,score])\n",
    "    \n",
    "    print(f\"KS: {ks.statistic:.4f} (p-value: {ks.pvalue:.3e})\")\n",
    "\n",
    "    return ks.statistic\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# By itself, Kolmogorov-Smirnov (KS) is a good measure of the degree of separation between the positive and negative \n",
    "# distributions that the model can achieve, for example deafult vs. non-default. However, it is important to monitor any \n",
    "# changes in KS from the baseline period and current monitoring period. This is typically done by looking at the relative \n",
    "# percentage change. For example if KS in the baseline is 80%, or 0.80, and in the current period it is 0.60, then \n",
    "# (0.60 - 0.80)/).80 = -25%. The inner and outer thresholds for this Key Performance Indicator would be -15% to -25%.\n",
    "\n",
    "def datga_drift_ks_test(model_score_target_baseline, model_score_target_current, score, target):\n",
    "    '''\n",
    "    Args:\n",
    "        model_score_target_baseline: dataframe of the model output during baseline and binary target (1=event, 0=non event), with the colunns \n",
    "        labeled to match the target_column and score_column strings passed in\n",
    "        model_score_target_current: datafrmae of the model output during current monitoring period\n",
    "        target: string of target column in the dataframe, example \"y\"\n",
    "        score: string of the score, or model output, typically [0,100], where the higher the score the higher\n",
    "        the likelihood of an event, for example \"score\"\n",
    "       \n",
    "    Returns:\n",
    "       ks_relative_percent_change: the KS value for the cummulative distribution function of the scores of events and non-events\n",
    "    '''\n",
    "    ks_baseline=ks_2samp(model_score_target_baseline.loc[model_score_target_baseline[target]==0,score], \n",
    "                model_score_target_baseline.loc[model_score_target_baseline[target]==1,score])\n",
    "    ks_current=ks_2samp(model_score_target_current.loc[model_score_target_current[target]==0,score], \n",
    "                model_score_target_current.loc[model_score_target_current[target]==1,score])\n",
    "    \n",
    "    print(f\"KS Baseline: {ks_baseline.statistic:.4f} (p-value: {ks_baseline.pvalue:.3e})\")\n",
    "    print(f\"KS Monitoring Period: {ks_current.statistic:.4f} (p-value: {ks_current.pvalue:.3e})\")\n",
    "    \n",
    "    ks_relative_percent_change = (ks_current.statistic - ks_baseline.statistic)/ks_baseline.statistic\n",
    "\n",
    "    return ks_relative_percent_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rDXnNSIt5GUs",
   "metadata": {
    "id": "rDXnNSIt5GUs"
   },
   "source": [
    "# Stage1. Business Case \n",
    "\n",
    "Stage one of model development is the development of the business case; why we need a model, the application, who would use it and how, and what the relative costs and banafits are for it.\n",
    "\n",
    "T\n",
    "\n",
    "\n",
    "Limitations of the models\n",
    "\n",
    "Assumptions made by the models\n",
    "\n",
    "Restrictions of the models...\n",
    "\n",
    "@@@@@@@@@@@@@@@@@@@@@ MADHU ARE YOU OK DEFINING KPIs HERE? 1) ACCURACY? 2) FAIRNESS\n",
    "\n",
    "#Added by Madhu\n",
    "Since any machine learning model accuracy is never 100%, the model might err and deny credit to a qualified application.  A fair model would not disparately err in favor of one population groups versus the others. Whilt is most ofte increasing the model accuracy seems to be the most pertinent metric for model design, it is most often seen that such model might not result in fair responses t certain population groups.  \n",
    "\n",
    "In case of a credit approval process, false negatives are detrimental from the perspective of the applicant, as this will deny approval to a qualified applicant.  Recall is a measure that can be used to ensure that the false negatives are consistent across various groups.  Alternatively, from the lending institution perspective, false positives are detrimental.  This means that model precision should be maximized, which would indicate minimal false positives.\n",
    "\n",
    "The process to identify an appropriate model in this context would be to review recall values that are within acceptable limits for different population groups, concurrently maximizing the overall model precision. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_DfUFfsmFWh5",
   "metadata": {
    "id": "_DfUFfsmFWh5"
   },
   "source": [
    "# Stage2. Data\n",
    "\n",
    "Next we load the data we need to achieve our business aims, wrangle it and prepare it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z_M1xrqYGCJ9",
   "metadata": {
    "id": "z_M1xrqYGCJ9"
   },
   "source": [
    "## Stage2a. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sFBrITKU_6A0",
   "metadata": {
    "id": "sFBrITKU_6A0"
   },
   "source": [
    "If this is running in Google Colab.... we extract the GitHub loc where the data resides...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CysAAt4-AdN8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CysAAt4-AdN8",
    "outputId": "5df1733d-cc12-4cff-c031-c9e38f741495"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/danphilps/credit_use_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LusHe3a2BF_t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LusHe3a2BF_t",
    "outputId": "32b4acf2-57da-4000-808d-0024130aa733"
   },
   "outputs": [],
   "source": [
    "# We should see a \"credit_use_case\" directory....\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hhMDMb1xB0yk",
   "metadata": {
    "id": "hhMDMb1xB0yk"
   },
   "outputs": [],
   "source": [
    "os.chdir('credit_use_case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b1a44",
   "metadata": {
    "id": "c26b1a44"
   },
   "outputs": [],
   "source": [
    "loc = \"credit-g.csv\"\n",
    "df_raw = pd.read_csv(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pwvlymBMFqsW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwvlymBMFqsW",
    "outputId": "8bf75dad-d73b-490e-e8b0-8fec8e344f00"
   },
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cB4L-E2-Te1u",
   "metadata": {
    "id": "cB4L-E2-Te1u"
   },
   "source": [
    "There is an imbalance in the dataset, and it is possible that it could create biases in our model. **We will come back to this issue later**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tVRDgt03tG5c",
   "metadata": {
    "id": "tVRDgt03tG5c"
   },
   "source": [
    "### Check for proxies of our protected characteristics\n",
    " \n",
    "We noted that there is a bias in our dataset between the protected characterisctis of gender, with male and female credits being different, and we removed this protected characteristic from the dataset to avoid this illegal bias in loan approval outcomes.\n",
    "\n",
    "Protected characteristics can be picked up (proxied) in other dataitems in more subtle ways though. For instance given that single parent households tend to disproportionately be led by a female adult, this may make 'num_dependents' a proxy for gender.\n",
    "\n",
    "It is important that we control for any possible protected biases, and one way of achieving this is to retrain our model using a mitigator, which trains by constraining the model weights to produce a balanced outcome between protected classes; male and female credits in this case.\n",
    "\n",
    "To ascertain proxies we can check the correlation of our protected feature with ther features in the dataset.\n",
    "\n",
    "Our protected features is categorical, so compare other categorical features with our protected feature we can use a Chi2 test. \n",
    "To compare  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b5f4a-337e-4191-b19b-b784d8c60ea8",
   "metadata": {},
   "source": [
    "In the current context, the machine learning methodology uses the training data to identify the best fitting model. The metrics calculated as a part of full test/validation of the machine learning model may not provide an accurate picture of how fair the model is. When the same model is run with a subset of test data for only a specific population group (e.g. for female and male applications), the model performance for each group might be completely different.  \n",
    "\n",
    "An objective of a fair model is to ensure that the performance differences are not too different for each of the group. \n",
    "\n",
    "In case of a credit approval process, false negatives are detrimental from the perspective of the applicant, as this will deny approval to a qualified applicant.  Recall is a measure that can be used to ensure that the false negatives are consistent across various groups.  Alternatively, from the lending institution perspective, false positives are detrimental.  This means that model precision should be maximized, which would indicate minimal false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eButjMDcPwHz",
   "metadata": {
    "id": "eButjMDcPwHz"
   },
   "outputs": [],
   "source": [
    "# MADHU: FAIRNESS FUNCTION HERE!!\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "\n",
    "# Fairness: run the model on different groups, and get precision, accuracy, f1 and so on, for each model run/group\n",
    "# Calcluates these stats for the predictions of a trained model (mod) for each category \n",
    "# in a given column (category_col_name) in the data set (X_test). \n",
    "def fairness_stats_get (mod: object, \n",
    "                      X_test: pd.DataFrame, \n",
    "                      y_test: pd.DataFrame, \n",
    "                      X_test_category_col: pd.DataFrame,\n",
    "                      y_approval_threshold: float = 0.5) -> pd.DataFrame:\n",
    "   \n",
    "  '''    \n",
    "  Args:\n",
    "      mod: sklearn model, trained without the category_col_name, and ready to test for biases.\n",
    "      X_test: X data, including the category_col_name you want to examine \n",
    "      y_test: y data, including the category_col_name you want to examine\n",
    "      X_test_category_col: column - corresponding to X_test and y_test in which categories are contained we want to test for fairness\n",
    "      y_approval_threshold: We are forevcasting the probability of default, this is the threashold over which we assume credit will be offered\n",
    "      \n",
    "  Returns:\n",
    "      df_stats: record of the accuracy (etc) of the model on each category. Examine this for fairness...\n",
    "\n",
    "  Author:\n",
    "    Madhu Nagarajan\n",
    "  '''\n",
    "  #Sanity\n",
    "  if mod is None:\n",
    "    raise TypeError('mod has not been instantiated or trained')\n",
    "  if X_test.shape[0] != y_test.shape[0]:\n",
    "    raise TypeError('X_test.shape[0] != y_test.shape[0]')\n",
    "  if X_test.shape[0] != X_test_category_col.shape[0]:\n",
    "    raise TypeError('X_test.shape[0] != X_test_category_col.shape[0]:')\n",
    "  if (y_approval_threshold < 0) | (y_approval_threshold > 1):\n",
    "    raise TypeError('(approval_threshold < 0) | (approval_threshold > 1)')\n",
    "\n",
    "  # Ini\n",
    "  df_stats = pd.DataFrame()\n",
    "  stats_cols = []\n",
    "\n",
    "  # Get categories in our test column\n",
    "  categories = X_test_category_col.unique()\n",
    "\n",
    "  # Loop through each of the categories in the category_col\n",
    "  # Eg male (=0) and female (=1)\n",
    "  # test acccuracy/precision/recall for each cat\n",
    "  for cat in categories:\n",
    "      print (cat)\n",
    "      #Filter on the cat\n",
    "      cat_rows = (X_test_category_col[cat])\n",
    "      #X...  \n",
    "      print (cat_rows)\n",
    "      X_test_cat = X_test.loc[cat_rows]\n",
    "      #y...\n",
    "      y_test_cat = y_test.loc[cat_rows]\n",
    "      \n",
    "      # Predict the probability of default, and decide who to offer credit to, for specific population groups\n",
    "      y_test_cat_hat_pred_proba = mod.predict_proba(X_test_cat)\n",
    "      y_test_cat_hat = (y_test_cat_hat_pred_proba[:,0] < y_approval_threshold).astype('int')\n",
    "\n",
    "      # Calc and record fairness analytics for each cat, record in df_stats\n",
    "      TN, FP, FN, TP = confusion_matrix(y_test_cat, y_test_cat_hat, labels = [0,1]).ravel()\n",
    "      fnr = FN/(FN+TP)\n",
    "      fdr = FP/(FP+TP)\n",
    "      fpr = FP/(FP+TN)\n",
    "      npv = TN/(TN+FN)\n",
    "\n",
    "      cat_row = pd.Series({'cat': cat,\n",
    "                          'cat_proportion': np.divide(float(X_test_cat.shape[0]),float(X_test.shape[0])), \n",
    "                          'accuracy': accuracy_score(y_true=y_test_cat, y_pred=y_test_cat_hat),  \n",
    "                          'precision': precision_score(y_true=y_test_cat, y_pred=y_test_cat_hat), \n",
    "                          'recall': recall_score(y_true=y_test_cat, y_pred=y_test_cat_hat), \n",
    "                          'fnr': fnr, 'fdr': fdr, 'fpr': fpr, 'npv': npv})\n",
    "\n",
    "      # Build record of accuracy and so on, of each category in the category_col\n",
    "      df_stats = pd.concat([df_stats, cat_row], axis=1)\n",
    "      stats_cols.append(cat)\n",
    "    \n",
    "    # Predict the probability of default, and decide who to offer credit to, for all data\n",
    "  y_test_pred_proba = mod.predict_proba(X_test)\n",
    "  y_test_hat = (y_test_pred_proba[:,0] < y_approval_threshold).astype('int')\n",
    "\n",
    "  # Calc and record fairness analytics for each cat, record in df_stats\n",
    "  TN, FP, FN, TP = confusion_matrix(y_test, y_test_hat, labels = [0,1]).ravel()\n",
    "  fnr = FN/(FN+TP)\n",
    "  fdr = FP/(FP+TP)\n",
    "  fpr = FP/(FP+TN)\n",
    "  npv = TN/(TN+FN)\n",
    "\n",
    "  cat_row = pd.Series({'cat': \"All\",\n",
    "                      'cat_proportion': 1, \n",
    "                      'accuracy': accuracy_score(y_true=y_test, y_pred=y_test_hat),  \n",
    "                      'precision': precision_score(y_true=y_test, y_pred=y_test_hat), \n",
    "                      'recall': recall_score(y_true=y_test, y_pred=y_test_hat), \n",
    "                      'fnr': fnr, 'fdr': fdr, 'fpr': fpr, 'npv': npv})\n",
    "\n",
    "  # Build record of accuracy and so on, of each category in the category_col\n",
    "  df_stats = pd.concat([df_stats, cat_row], axis=1)\n",
    "  stats_cols.append(cat)  \n",
    "    \n",
    "  \n",
    "  # Set up df_stats with column names and an index\n",
    "  df_stats = df_stats.transpose()\n",
    "  df_stats.columns = cat_row.index\n",
    "  df_stats = df_stats.set_index(df_stats['cat'])\n",
    "  #df_stats = df_stats.drop('cat', axis=1)\n",
    "\n",
    "  return df_stats\n",
    "\n",
    "\n",
    "\n",
    "def plot_fairness_charts (df_stats: pd.DataFrame, \n",
    "                      majority_class: str = \"Male\", \n",
    "                      fairness_metric: str = [\"recall\", \"precision\"],\n",
    "                      y_approval_threshold: float = 0.5) -> pd.DataFrame:\n",
    "   \n",
    "    '''    \n",
    "    Args:\n",
    "        df_stats: record of the accuracy (etc) of the model on each category.\n",
    "        majority_class: string - the value of the majority class against which the other population groups are compared with (e.g. [\"Male\"])\n",
    "        fairness_metric: array with the fairness metrics to compare e.g. [\"recall\"]\n",
    "        \n",
    "    Returns:\n",
    "        A plot charting the fairness metric values to the various population groups...\n",
    "    \n",
    "    Author:\n",
    "      Madhu Nagarajan\n",
    "    '''\n",
    "    \n",
    "    sub_plot_id = 0\n",
    "    #Get the set of population group items from the stats table. The stats table contain the matrics, one row for eacg group (e.g. male/female)\n",
    "    X_val = df_stats[\"cat\"].values.tolist()\n",
    "    #Iterate for each metric (e.g. recall) provided as input parameter\n",
    "    for ametric in fairness_metric: \n",
    "        sub_plot_id = sub_plot_id + 1\n",
    "        plt.subplot(1,len(fairness_metric),sub_plot_id)\n",
    "        \n",
    "        #Get the metric corresponding to the majority - e.g. the recall corresponding to Male group\n",
    "        majority_class_metric  = df_stats.loc[df_stats[\"cat\"] == majority_class, ametric].astype('float64')\n",
    "        \n",
    "        #Y values to plot are the metrics of population groups, get them from df_stats\n",
    "        Y_val = df_stats[ametric].values.tolist()\n",
    "        \n",
    "        #build a bar chart\n",
    "        plt.bar(X_val,Y_val) \n",
    "        plt.ylabel(ametric)\n",
    "        \n",
    "        #The plot displays a range that is +/- 20% from the metric for the majority class\n",
    "        plt.axhline(y=majority_class_metric.values[0]*0.8,color='red')\n",
    "        plt.axhline(y=majority_class_metric.values[0],color='green')\n",
    "        plt.axhline(y=majority_class_metric.values[0]*1.2,color='red')\n",
    "        plt.ylim(0,1)\n",
    "        plt.ylabel(ametric)\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd49632",
   "metadata": {
    "id": "bdd49632"
   },
   "source": [
    "## Stage2b. Data Wrangling and Preprocessing\n",
    "\n",
    "Data Wrangling: As we have shown in previous chapters we need to convert categorical data into one-hot-encodings, clean characters from numeric data columns, carry out type conversions into numeric datatypes... The following cell shows the appropriate data wrangling to get our data into a good shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62115a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "c62115a2",
    "outputId": "48adb4b5-b2a9-4e95-9b24-8b56350895d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copt raw data into our df\n",
    "df = df_raw.copy(deep=True)\n",
    "\n",
    "\n",
    "# list categorical columns...\n",
    "cat_cols =['checking_status', 'purpose', 'credit_history', 'savings_status', 'employment', 'other_parties', 'property_magnitude', 'other_payment_plans','housing','job','own_telephone']\n",
    "\n",
    "# One hot encoding of catagorical variables...\n",
    "df[cat_cols].astype(\"category\")\n",
    "one_hot_encodings = pd.get_dummies(df[cat_cols])\n",
    "\n",
    "#Combine dfs\n",
    "df = pd.concat([df, one_hot_encodings], axis=1)\n",
    "\n",
    "#remove categorical columns...\n",
    "df = df.drop(columns=cat_cols)\n",
    "\n",
    "#Data wrangling..... get types and bad values sorted out\n",
    "\n",
    "# Remove characters in numeric columns (note that the data type was set, by the open_csv function to object, \n",
    "# so we first convert to string before running the replace function (which can only be fun on str types)\n",
    "df['foreign_worker'] = df['foreign_worker'].str.replace('yes', '1')\n",
    "df['foreign_worker'] = df['foreign_worker'].str.replace('no', '0')\n",
    "df['class'] = df['class'].str.replace('good', '0')\n",
    "df['class'] = df['class'].str.replace('bad', '1')\n",
    "\n",
    "# Convert to numerics so we can use in ML... we force type conversions, then print our resulting df.\n",
    "df['foreign_worker'] = pd.to_numeric(df['foreign_worker'], errors ='coerce').fillna(0).astype('int')\n",
    "df['class'] = pd.to_numeric(df['class'], errors ='coerce').fillna(0).astype('int')\n",
    "df['duration'] = pd.to_numeric(df['duration'], errors ='coerce').fillna(0).astype('int')\n",
    "df['installment_commitment'] = pd.to_numeric(df['installment_commitment'], errors ='coerce').fillna(0).astype('int')\n",
    "df['residence_since'] = pd.to_numeric(df['residence_since'], errors ='coerce').fillna(0).astype('int')\n",
    "df['age'] = pd.to_numeric(df['age'], errors ='coerce').fillna(0).astype('int')\n",
    "df['num_dependents'] = pd.to_numeric(df['num_dependents'], errors ='coerce').fillna(0).astype('int')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jCs33N2k8iVd",
   "metadata": {
    "id": "jCs33N2k8iVd"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "BrwhvERWDe_N",
   "metadata": {
    "id": "BrwhvERWDe_N"
   },
   "source": [
    "## Stage2c. Exploratory Data Analysis\n",
    "\n",
    "Once we have loaded the data and have it in a useable form, we now need to examine it, to build an intuition for the distributions, accuracy, missing values, imbalances and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OKIZQIrJRJOF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "OKIZQIrJRJOF",
    "outputId": "8aba3feb-42ea-49b9-9fe1-e529d913b6bd"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "\n",
    "def correlation_between_features(df: pd.DataFrame):\n",
    "    '''\n",
    "    Args:\n",
    "        df: Dataframe of the features\n",
    "       \n",
    "    Returns:\n",
    "       None\n",
    "    '''\n",
    "\n",
    "    # Descriptive stats\n",
    "    display(df.describe())\n",
    "\n",
    "    # Correlation matrix\n",
    "    correlation_matrix = df.corr().abs()\n",
    "    sns.heatmap(correlation_matrix, annot=True)\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "# Run simple EDA function...\n",
    "correlation_between_features(df=df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd02130",
   "metadata": {
    "id": "1fd02130"
   },
   "source": [
    "One major bias, particularly when we are fitting a model to a relatively low probability event, such as a default, are imbalances in the data. By definition our target occurs less than half the time.\n",
    "\n",
    "#### Bias Alert: Imbalanced dataset\n",
    "\n",
    "Imbalances in datasets for classification problems are a big issue. We generally need to balance the dataset to contain an equal proportion of the different classes before training (and testing). For the credit use-case, we have two classes {1,0}, meaning that ideally 50% of our samples should be class=1; and 50% class=0. If this is not the case and we have an imbalance (we do), we can balance the data by up-sampling the minority class, or down-sampling the majority class.\n",
    "Let us first examine the dataset to determine whether it is in balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d772e1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "9d772e1c",
    "outputId": "6022036a-a819-4211-ede6-043b5a2dfac8"
   },
   "outputs": [],
   "source": [
    "# Imbalanced y classes?\n",
    "imbalanced_y_check(df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0459bd5",
   "metadata": {
    "id": "e0459bd5"
   },
   "source": [
    "The dataset is not balanced. 70% of samples are class=0; only 30% are class=1. We should bring this into balance before we train our model, or risk introducing dangerous biases into our forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qV1jmvnXDdEl",
   "metadata": {
    "id": "qV1jmvnXDdEl"
   },
   "source": [
    "#### Bias Alert: Protected and Majority Groups...\n",
    "\n",
    "Loan approval is a high risk application. We need to go very carefully as a result. We need to identify any protected characteristics (ie it would be illegal to differentiate based on these) present in the dataset. \n",
    "Our use-case is a loan approval use case, ethnicity and gender are protected charcteristics where biases would be illegal. We clearly need to remove these features.\n",
    "It is also possible to identify majority groups and monitor and control for  biases in our model based on these characterictics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NQaeBvyxMddk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NQaeBvyxMddk",
    "outputId": "1038df2f-1dd5-4168-de5b-35d06179aad9"
   },
   "outputs": [],
   "source": [
    "# Check for biases on protected characteristics...\n",
    "df_female = df_raw[(df_raw['personal_status'].str.contains('female') != 0)]\n",
    "df_male = df_raw[(df_raw['personal_status'].str.contains('female') == 0)]\n",
    "\n",
    "# % of females \n",
    "female_good_credits = df_female[(df_female['class'] == 'good')].shape[0]\n",
    "female_good_credits_pct = female_good_credits / df_female.shape[0]\n",
    "#\n",
    "male_good_credits = df_male[(df_male['class'] == 'good')].shape[0] \n",
    "male_good_credits_pct = male_good_credits / df_male.shape[0]\n",
    "\n",
    "#  Difference in good credits for females and males...\n",
    "print('Female good credits: ' + str(format(round(female_good_credits_pct*100, 2))) + '%')\n",
    "print('Male good credits: ' + str(format(round(male_good_credits_pct*100, 2))) + '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35GhK1unDr81",
   "metadata": {
    "id": "35GhK1unDr81"
   },
   "source": [
    "Having found some protected characteristics and potential biases, we need to refine the dataset to consider these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0n2YS32BDd6N",
   "metadata": {
    "id": "0n2YS32BDd6N"
   },
   "outputs": [],
   "source": [
    "# Protected characteristics - 'personal_status' has a \"gender\" classifier - We need to remove this from model training.\n",
    "df['gender'] = np.where(df['personal_status'].str.contains('female') == 0, \"Female\", \"Male\") #'male'=0; female=1\n",
    "df = df.drop('personal_status', axis=1)\n",
    "\n",
    "# Another protected characteristic is age. We can categorise the ages in our dataset... \n",
    "df['age'] = df['age'].apply(lambda x: 3 if x > 65 else (2 if x > 25 else 1))\n",
    "\n",
    "# **********************************************************\n",
    "# We need to remove these columns from model training, or risk \n",
    "# illegal biases in our outcomes... \n",
    "# Keep a record of the protected columns\n",
    "protected_cols = ['gender', 'age']\n",
    "# **********************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9Vx9c_4sStE",
   "metadata": {
    "id": "i9Vx9c_4sStE"
   },
   "source": [
    "# Stage3: Model Design \n",
    "\n",
    "Now we need to take what we have learned about the data, and find an appropriate model to achieve our KPIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ubSKhBH-pg",
   "metadata": {
    "id": "c0ubSKhBH-pg"
   },
   "source": [
    "## Stage 3a: Test the performance of different up, and down sampling approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e286323c",
   "metadata": {
    "id": "e286323c"
   },
   "source": [
    "Before we start up or down-sampling to correct the imbalance, we first we need to create our testing and training datasets. We can then balance the training set. This is to keep the training-set in-sample and the testing-set strictly out-of-sample. There is a risk of data leakage in this process we must consider and contriol for.\n",
    "\n",
    "#### Bias Alert: Data Leakage \n",
    "\n",
    "Separate training and testing datasets BEFORE balancing the dataset is crucial to avoid adta leakage. This is crucial as our learner must not see any of the test samples until we actually test it for performance. If we fail to separate testing and training data before up samplng, we can suffer data-snooping biases (also called data-leakage), which would invalidate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eTjDqiD5f_j1",
   "metadata": {
    "id": "eTjDqiD5f_j1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9517124",
   "metadata": {
    "id": "f9517124"
   },
   "outputs": [],
   "source": [
    "# Define X and y variables\n",
    "cols  = list(df.columns)\n",
    "cols.remove('class')\n",
    "\n",
    "# Contains only numerics\n",
    "X = df[cols]\n",
    "y = df['class']\n",
    "\n",
    "#Test and train set    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "#****************************\n",
    "# Keep record of protected for bias testing later\n",
    "X_train_protected = X_train[protected_cols]\n",
    "X_test_protected = X_test[protected_cols]\n",
    "\n",
    "# Remember to remove protected columns before training\n",
    "X_train = X_train.drop(protected_cols, axis=1)\n",
    "X_test = X_test.drop(protected_cols, axis=1)\n",
    "#****************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f79ba0",
   "metadata": {
    "id": "d3f79ba0"
   },
   "source": [
    "### Stage Test up and down sampling approaches using a Random Forest Classifier\n",
    "We will use the RandomForest classifier to help us test different up and down sampling approaches to deal with the imbalanced dataset. We will be able to see the relative performance of each balancing approach on our problem.\n",
    "\n",
    "First let us run the classifier on the imbalanced data and examine the F1 score that results when we test the model. (It is a very poor result)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02db5bb",
   "metadata": {
    "id": "c02db5bb"
   },
   "source": [
    "Get the sklearn packages we will need for our clasification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af12e96b",
   "metadata": {
    "id": "af12e96b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# metrics...\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7484849",
   "metadata": {
    "id": "c7484849"
   },
   "source": [
    "We can wrap training for the Random Forest classifier, and the printing of performance metrics in a function, as we will be running this more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b7144",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "id": "c09b7144",
    "outputId": "2fc9b5ad-55e2-47ae-d527-d804a33d1159"
   },
   "outputs": [],
   "source": [
    "# Declare a function to wrap training of a classifier and printing of performance data\n",
    "def run_rf_classification_models(X_train: pd.DataFrame, \n",
    "                              X_test: pd.DataFrame, \n",
    "                              y_train: pd.DataFrame, \n",
    "                              y_test: pd.DataFrame) -> object:\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "      X_train: DataFrame with training data for classifier, columns are features, rows are instances\n",
    "      X_test: Test data matching above shape\n",
    "      y_train: training data target variable {1,0}, instances are rows.\n",
    "      y_test: test data target variable {1,0}, instances are rows.\n",
    "       \n",
    "    Returns:\n",
    "       rf: sklearn model object\n",
    "       \n",
    "    Author:\n",
    "       Dan Philps\n",
    "    '''\n",
    "\n",
    "    #sanity\n",
    "    if X_train.shape[0] != y_train.shape[0]:\n",
    "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
    "    if X_test.shape[0] != y_test.shape[0]:\n",
    "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
    "    if (X_train.dtypes != X_test.dtypes).sum() != 0:\n",
    "      raise TypeError('Bad parameter: X_train.dtype != X_test.dtype')\n",
    "    if (y_train.dtypes != y_test.dtypes):\n",
    "      raise TypeError('Bad parameter: y_train.dtype != y_test.dtype')\n",
    "\n",
    "    # Scale and transform the data for training\n",
    "    sclr = StandardScaler()\n",
    "    sclr.fit(X_train) # scale to 0 mean and std dev 1 on training data\n",
    "\n",
    "    X_train = sclr.fit_transform(X_train) # scale both sets:\n",
    "    X_test = sclr.fit_transform(X_test)\n",
    "\n",
    "    # classifier train\n",
    "    rf = RandomForestClassifier(max_depth=5,random_state=0)\n",
    "    rf.fit(X_train,y_train)\n",
    "    y_train_hat =rf.predict(X_train)\n",
    "    y_test_hat = rf.predict(X_test)\n",
    "\n",
    "    # Analytics calculated wrt default or y=1... Print score\n",
    "    print(type(rf))        \n",
    "    print(f\"Accuracy train: {rf.score(X_train,y_train):.4f}, test: \",\n",
    "      f\"{rf.score(X_test,y_test):.4f}\")\n",
    "    print(f\"Precision train: {precision_score(y_train, y_train_hat, average=None)[1]:.4f}, test: \",\n",
    "      f\"{precision_score(y_test,y_test_hat, average=None)[1]:.4f}\")\n",
    "    print(f\"Recall train: {recall_score(y_train, y_train_hat, average=None)[1]:.4f}, test: \",\n",
    "      f\"{recall_score(y_test,y_test_hat, average=None)[1]:.4f}\")\n",
    "    print(f\"F1 train: {f1_score(y_train, y_train_hat, average=None)[1]:.4f}, test: \",\n",
    "      f\"{f1_score(y_test,y_test_hat, average=None)[1]:.4f}\")\n",
    "    \n",
    "    #Print confusion matrix...\n",
    "    cf_matrix = confusion_matrix(y_test, y_test_hat, labels=[0, 1]) \n",
    "    cf_matrix_norm = cf_matrix.astype('float') # / cf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    ax = sns.heatmap(cf_matrix_norm, annot=True, cmap='Blues', fmt='g')\n",
    "    ax.set_title('Confusion Matrix\\n\\n');\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "    plt.show()\n",
    "\n",
    "    #sanity\n",
    "    if rf is None:\n",
    "      raise TypeError('Bad return: rf is None')\n",
    "\n",
    "    return rf\n",
    "\n",
    "#run our classifier function\n",
    "mod = run_rf_classification_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print('Now use sklearns predict_proba to generate probability values for each prediction')\n",
    "mod.predict_proba(X_test)[0:9,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8166fcf",
   "metadata": {
    "id": "f8166fcf"
   },
   "source": [
    "### 3a i) Upsampling using resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef22f5",
   "metadata": {
    "id": "a4ef22f5"
   },
   "source": [
    "First we test up-sampling using sklearn's resample, and examine how well it does using the RandomForest classifier. Resampling can up-sample by simply randomly selecting and copying existing observations of the minority class. We can balance the classes in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e0d7d",
   "metadata": {
    "id": "739e0d7d"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "#Split first to avoid data-snooping\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "# Create up-sampled data set for minority class\n",
    "# note that n_samples= the number of samples the imbalance represents.\n",
    "X_upsampled, y_upsampled = resample(X_train[y_train == 1],\n",
    "                                        y_train[y_train == 1],\n",
    "                                        replace=True,\n",
    "                                        n_samples=(X_train[y_train == 0].shape[0]-X_train[y_train == 1].shape[0]),\n",
    "                                        random_state=None)\n",
    "\n",
    "#****************************\n",
    "# Keep record of protected for bias testing later\n",
    "X_upsampled_protected = X_upsampled[protected_cols]\n",
    "X_train_protected = X_train[protected_cols]\n",
    "X_test_protected = X_test[protected_cols]\n",
    "\n",
    "# Remember to remove protected columns before training\n",
    "X_upsampled = X_upsampled.drop(protected_cols, axis=1)\n",
    "X_train = X_train.drop(protected_cols, axis=1)\n",
    "X_test = X_test.drop(protected_cols, axis=1)\n",
    "#****************************\n",
    "\n",
    "#Combine train with upsampled\n",
    "X_upsampled = X_train.append(X_upsampled)\n",
    "y_upsampled = y_train.append(y_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e5726",
   "metadata": {
    "id": "fc1e5726"
   },
   "source": [
    "Let us check everything is in balance now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb61d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "b8cb61d7",
    "outputId": "11cd623f-8058-498b-f881-2079935c0812"
   },
   "outputs": [],
   "source": [
    "# Imbalanced y classes?\n",
    "temp = pd.concat([X_upsampled, y_upsampled], axis=1)\n",
    "kpi_imbalanced_y_check(temp['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dea8e1",
   "metadata": {
    "id": "53dea8e1"
   },
   "source": [
    "Dataset is perfectly in balance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caebbb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "5caebbb0",
    "outputId": "bb6602b7-bd06-46d3-e76f-f2595119f6f6"
   },
   "outputs": [],
   "source": [
    "#Run our function....\n",
    "model = run_rf_classification_models(X_upsampled, X_test, y_upsampled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4afcc1",
   "metadata": {
    "id": "1b4afcc1"
   },
   "source": [
    "The F1 score on the test data has increased markedly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b384e",
   "metadata": {
    "id": "e98b384e"
   },
   "source": [
    "### 3a ii) Up-sampling using a synthetic over sampling approach called SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235505f6",
   "metadata": {
    "id": "235505f6"
   },
   "source": [
    "Add the libraries we will need... and generate the synthetic data to balance our classes using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadf40d",
   "metadata": {
    "id": "5dadf40d"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#How many samples do we need to balance?\n",
    "idx = np.random.choice(X_train.shape[0], size=X_train[y_train == 0].shape[0]-X_train[y_train == 1].shape[0], replace=False)\n",
    "\n",
    "# Generate SMOTE samples and use this to train\n",
    "upsampler_smote = SMOTE()\n",
    "X_upsampled_smote, y_upsampled_smote = upsampler_smote.fit_resample(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JQKlvmapJgqT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "JQKlvmapJgqT",
    "outputId": "1804b7f7-2436-4a69-8c0e-b65ae7a0fa25"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14d264",
   "metadata": {
    "id": "4a14d264"
   },
   "source": [
    "Before we use the up-sampled dataset to train our classifier, let us first examine the distribution of the synthetic datapoints that SMOTE creates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff54d69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "4ff54d69",
    "outputId": "77e4b702-4129-445b-b500-1e95af1fb6df"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7)) \n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_train['credit_amount'],X_train['installment_commitment'], X_train['duration'], marker=\"o\", s=10, c='blue', label='Real datapoints')\n",
    "ax.scatter(X_upsampled_smote['credit_amount'],X_upsampled_smote['installment_commitment'], X_upsampled_smote['duration'], marker=\"+\", s=50, c='red', label='SMOTE datapoints')\n",
    "\n",
    "# set axes range\n",
    "plt.xlim(-500, 11000)\n",
    "plt.ylim(0, 40)\n",
    "\n",
    "ax.set_xlabel('credit_amount')\n",
    "ax.set_ylabel('installment_commitment')\n",
    "ax.set_zlabel('duration')\n",
    "\n",
    "plt.title('How SMOTE Samples are Distributed vs Real Data Points')\n",
    "plt.legend(loc=1,framealpha=1, fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875103b",
   "metadata": {
    "id": "a875103b"
   },
   "source": [
    "The synthetic datapoints look realistic at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031433f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "c031433f",
    "outputId": "d02c35bc-39b4-4fde-ceae-9ca73c514c16"
   },
   "outputs": [],
   "source": [
    "# Imbalanced y classes?\n",
    "temp = pd.concat([X_upsampled_smote, y_upsampled_smote], axis=1)\n",
    "kpi_imbalanced_y_check(temp['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8fdc6",
   "metadata": {
    "id": "89d8fdc6"
   },
   "source": [
    "After up-sampling using SMOTE's synthetic data, the dataset is perfectly in balance... however we have a Fairness problem. Can you spot what it is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a6983",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "750a6983",
    "outputId": "487bbd6c-ef95-465b-d54b-6f831e93bcb1"
   },
   "outputs": [],
   "source": [
    "#Run our function....\n",
    "model = run_rf_classification_models(X_upsampled_smote, X_test, y_upsampled_smote, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kHUCz-3ksp6r",
   "metadata": {
    "id": "kHUCz-3ksp6r"
   },
   "source": [
    "### 3a iii) Down-sampling - Removing rows to balance the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94d7e4",
   "metadata": {
    "id": "0a94d7e4"
   },
   "source": [
    "Now we can test down-sampling, which is simply removing samples from the majority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc9854",
   "metadata": {
    "id": "8dbc9854"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "\n",
    "#Split first to avoid data-snooping\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "#****************************\n",
    "# Keep record of protected for bias testing later\n",
    "X_train_protected = X_train[protected_cols]\n",
    "X_test_protected = X_test[protected_cols]\n",
    "\n",
    "# Remember to remove protected columns before training\n",
    "X_train = X_train.drop(protected_cols, axis=1)\n",
    "X_test = X_test.drop(protected_cols, axis=1)\n",
    "#****************************\n",
    "\n",
    "# Randomly downsample rows in the majority class\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_downsampled, y_downsampled = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0beacf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "c0beacf7",
    "outputId": "6d1f3d5b-b7af-443b-8a11-314ef1a540ac"
   },
   "outputs": [],
   "source": [
    "# Imbalanced y classes?\n",
    "temp = pd.concat([X_downsampled, y_downsampled], axis=1)\n",
    "kpi_imbalanced_y_check(temp['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de0276",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "d6de0276",
    "outputId": "c842b047-3ac6-4f82-98c9-438c3a266ac3"
   },
   "outputs": [],
   "source": [
    "# Run our function to train a rf classifier....\n",
    "model = run_rf_classification_models(X_downsampled, X_test, y_downsampled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ea491",
   "metadata": {
    "id": "5e6ea491"
   },
   "source": [
    "As we can see, all up and down sampling approaches have outperformed the F1 Score on the imbalanced data. The most impressive performance in this case is from up-sampling using SMOTE synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfe3c2",
   "metadata": {
    "id": "66bfe3c2"
   },
   "source": [
    "## Stage 3b: Model Selection\n",
    "\n",
    "\n",
    "Next we need to select the type of classifier we are going to use to model our problem and separate good from bad credits. We could just make a heuristic selection, but we are likely to be biased in making this selection. \n",
    "\n",
    "#### Bias Alert: Availability Heuristic\n",
    "For instance,  is our quant team overpopulated by linear regression experts? Do we have a greater level of familiarity for Random Forest classifiers?\n",
    "\n",
    "One way of dealing with this is to use of develop a model selection approach. We can test the problem, using many different classsifiers and assess the performance of exach based on our KPIs. However, we must be careful as even this more exhaustive approach is also open to biases. For instance if we are selection models based on p-values, we would suffer from multiplicity bias where testing multiple approaches would exaggerate the significance of a success. \n",
    "\n",
    "#### Bias Alert: Multiplicity bias\n",
    "\n",
    "To control for this we would use a measure such as F1-score, and we would test this on test samples not used for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tUbobdmKR9Or",
   "metadata": {
    "id": "tUbobdmKR9Or"
   },
   "outputs": [],
   "source": [
    "# Get cross-validation set, broken out from training data, to select the classifier\n",
    "# objective is to keep the classifier choice in sample, so that running on the test data \n",
    "# will be a true out-of-sample test. No data leakage bias.\n",
    "\n",
    "# Test and train set    \n",
    "X_train_cv, X_cv, y_train_cv, y_cv = train_test_split(X_train, y_train, test_size=0.3, random_state=None)\n",
    "\n",
    "# Cross validation as a subset of \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)\n",
    "\n",
    "#****************************\n",
    "# Remember to remove protected columns before training\n",
    "X_train = X_train.drop(protected_cols, axis=1)\n",
    "X_test = X_test.drop(protected_cols, axis=1)\n",
    "#****************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5SUdXN-jhrza",
   "metadata": {
    "id": "5SUdXN-jhrza"
   },
   "source": [
    "Sklean provides many different classifiers and we will be testing a wide range to determine their accuracy on our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G3Dro5K_QkgZ",
   "metadata": {
    "id": "G3Dro5K_QkgZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classsifiers from sklearn\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Performance metrics...\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JElvKh2qh75k",
   "metadata": {
    "id": "JElvKh2qh75k"
   },
   "source": [
    "To address the availability heuristic we will now build a function that tests a number of different classifiers on our problem, and the one with the best accuracy on the cross-validation data, we will select as the \"best\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cyji4ydlQTVZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cyji4ydlQTVZ",
    "outputId": "01e18259-2c8d-435a-dd93-6d5a8ad15087"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Func to wrap up running these selected classification learners...\n",
    "# NOTE: to test the performance of the learners out-of-sample, we should use a cross-validation dataset\n",
    "# this is a hold back dataset and we will use our testing data to do this, in this case. \n",
    "def auto_classifier_selection(X_train: pd.DataFrame, \n",
    "                            X_cross_validation: pd.DataFrame, \n",
    "                            y_train: pd.DataFrame, \n",
    "                            y_cross_validation: pd.DataFrame) -> (object, list, list, list):\n",
    "    \n",
    "    '''\n",
    "    Args:\n",
    "      X_train: DataFrame with training data for classifier, columns are features, rows are instances\n",
    "      X_cross_validation: cross validation data matching above shape, used in model selection\n",
    "      y_train: training data target variable {1,0}, instances are rows.\n",
    "      y_cross_validation: test data target variable {1,0}, instances are rows, used in model selection\n",
    "       \n",
    "    Returns:\n",
    "       max_mdl: sklearn model object performing \"best\"\n",
    "       all_mdls: list of sklearn classifier objects trained\n",
    "       all_mdls_desc: list of description of the above model objects (elements corresponding to the above also)\n",
    "       all_mdls_prec: list of precision scores of the above model objects (elements corresponding to the above also)\n",
    "       \n",
    "    Author:\n",
    "       Dan Philps\n",
    "    '''\n",
    "    #sanity\n",
    "    if X_train.shape[0] != y_train.shape[0]:\n",
    "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
    "    if X_cross_validation.shape[0] != y_cross_validation.shape[0]:\n",
    "      raise TypeError('Bad parameter: X_train.shape[0] != y_train.shape[0]')\n",
    "    if (X_train.dtypes != X_cross_validation.dtypes).sum() != 0:\n",
    "      raise TypeError('Bad parameter: X_train.dtype != X_cross_validation.dtype')\n",
    "    if (y_train.dtypes != y_cross_validation.dtypes):\n",
    "      raise TypeError('Bad parameter: y_train.dtype != y_cross_validation.dtype')\n",
    "\n",
    "    #Balance training data....\n",
    "    # Generate SMOTE samples and use this to train\n",
    "    upsampler_smote = SMOTE()\n",
    "    X_upsampled_smote, y_upsampled_smote = upsampler_smote.fit_resample(X_train.values, y_train.values)\n",
    "\n",
    "    sclr = StandardScaler()\n",
    "    sclr.fit(X_train.values) # scale to 0 mean and std dev 1 on training data\n",
    "\n",
    "    X_train = sclr.fit_transform(X_upsampled_smote) # scale both sets:\n",
    "    X_cross_validation = sclr.fit_transform(X_cross_validation)\n",
    "    \n",
    "    # These are the classifiers we will select from...\n",
    "    dtc = DecisionTreeClassifier(max_depth=5) #If we allow endless depth we overfit\n",
    "    gnb = GaussianNB()\n",
    "    lr = LogisticRegression(max_iter=2000,random_state=0)\n",
    "    mlp = MLPClassifier(max_iter=2000,random_state=1, early_stopping=True) # MLP will tend to overfit unless we stop early   \n",
    "    rf = RandomForestClassifier(max_depth=3,random_state=0) # << artibitrary parameters, consider hyper parameter tuning.\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    ada = AdaBoostClassifier()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    knn = KNeighborsClassifier(n_neighbors=3) # << artibitrary parameters, consider hyper parameter tuning.\n",
    "    svc = SVC(kernel=\"rbf\", C=0.025, probability=True) # << artibitrary parameters, consider hyper parameter tuning.\n",
    "    nsvc = NuSVC(probability=True)\n",
    "    \n",
    "    all_mdls = [dtc, gnb, lr, mlp, rf, lda, qda, ada, gbc, knn, svc, nsvc]\n",
    "    all_mdls_desc = ['dtc', 'gnb', 'lr', 'mlp', 'rf', 'lda', 'qda', 'ada', 'gbc', 'knn', 'svc', 'nsvc']\n",
    "    all_mdls_prec = []\n",
    "    \n",
    "    # Loop through each classifer and record the \"best\"...\n",
    "    max_prec = 0\n",
    "    for mdl in all_mdls:\n",
    "        #Fit model\n",
    "        mdl.fit(X_upsampled_smote,y_upsampled_smote)  \n",
    "        y_train_hat = mdl.predict(X_upsampled_smote)\n",
    "        y_cross_validation_hat = mdl.predict(X_cross_validation)       \n",
    "        \n",
    "        # Output model selection information....Analytics calculated wrt default or y=1... Print score\n",
    "        print(mdl)\n",
    "        print(f\"Precision train: {precision_score(y_upsampled_smote, y_train_hat, average=None)[1]:.4f}, cross-validation: \",\n",
    "        f\"{precision_score(y_cross_validation,y_cross_validation_hat, average=None)[1]:.4f}\")\n",
    "  \n",
    "        # Selection based on cross-validation set, ie out of sample data not used in training\n",
    "        this_cv_prec = precision_score(y_cross_validation,y_cross_validation_hat, average=None)[1]\n",
    "        if this_cv_prec > max_prec:\n",
    "            max_prec = this_cv_prec\n",
    "            max_mdl = mdl\n",
    "        \n",
    "        #Save the F1 score of this model...\n",
    "        all_mdls_prec.append(this_cv_prec)\n",
    "\n",
    "    # The best....\n",
    "    #Fit...\n",
    "    max_mdl.fit(X_upsampled_smote,y_upsampled_smote)\n",
    "    y_train_hat = max_mdl.predict(X_upsampled_smote)\n",
    "    y_cross_validation_hat = max_mdl.predict(X_cross_validation)\n",
    "    \n",
    "    # Analytics calculated wrt default or y=1... Print score\n",
    "    print('\\nWinner\\n', type(max_mdl))        \n",
    "    print(f\"Accuracy train: {max_mdl.score(X_train,y_upsampled_smote):.4f}, cross-validation: \",\n",
    "      f\"{max_mdl.score(X_cross_validation,y_cross_validation):.4f}\")\n",
    "    print(f\"Precision train: {precision_score(y_upsampled_smote, y_train_hat, average=None)[1]:.4f}, cross-validation: \",\n",
    "      f\"{precision_score(y_cross_validation,y_cross_validation_hat, average=None)[1]:.4f}\")\n",
    "    print(f\"Recall train: {recall_score(y_upsampled_smote, y_train_hat, average=None)[1]:.4f}, cross-validation: \",\n",
    "      f\"{recall_score(y_cross_validation,y_cross_validation_hat, average=None)[1]:.4f}\")\n",
    "    print(f\"F1 train: {f1_score(y_upsampled_smote, y_train_hat, average=None)[1]:.4f}, cross-validation: \",\n",
    "      f\"{f1_score(y_cross_validation,y_cross_validation_hat, average=None)[1]:.4f}\")\n",
    "        \n",
    "    #Print confusion matrix...\n",
    "    cf_matrix = confusion_matrix(y_cross_validation, y_cross_validation_hat, labels=[0, 1]) \n",
    "    cf_matrix_norm = cf_matrix.astype('float')\n",
    "\n",
    "    ax = sns.heatmap(cf_matrix_norm, annot=True, cmap='Blues', fmt='g')\n",
    "    ax.set_title('Confusion Matrix\\n\\n');\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "    plt.show()\n",
    "    \n",
    "    #sanity\n",
    "    if max_mdl is None:\n",
    "      raise TypeError('Bad return: max_mdl is None')\n",
    "\n",
    "    return max_mdl, all_mdls, all_mdls_desc, all_mdls_prec\n",
    "\n",
    "# Run our function....autoselect the best classifier wrt F1\n",
    "max_mdl, all_models, all_models_desc, all_mdls_prec = auto_classifier_selection(X_train_cv, X_cv, y_train_cv, y_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OHdKdhL6MF42",
   "metadata": {
    "id": "OHdKdhL6MF42"
   },
   "source": [
    "The output from our model selection function shows how each classifier performed on the training and cross-validation datasets. We can now test the \"winning classifier\" on our test data to ensure the cross-validation tests were effective in selecting a good performance out of sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BuJjXTsYMbBs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "BuJjXTsYMbBs",
    "outputId": "14615b41-ae89-44bd-db80-3fae0f26edc6"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_test_hat = max_mdl.predict(X_test.values)\n",
    "\n",
    "# Analyst KPI...\n",
    "f1, prec, rec = kpi_review_analyst(mdl=max_mdl,X=X_test, y=y_test[:].values, y_hat=y_test_hat[:])\n",
    "print(f1, prec, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RM6iBjWd_yEn",
   "metadata": {
    "id": "RM6iBjWd_yEn"
   },
   "source": [
    "#### Bias Alert: Inductive bias\n",
    "\n",
    "Even in the case where we select the most appropriate model, we may still be exposed to inductive biases. Each learner has a specific way that will approximate the function we are attempting to approximate, in this cadse the function of credit quality based on the characteristics of borrowers. Decision Trees, for instance, have inductive biases associated with greedy separation, whereas Random Forests (RF) mitigate this bias by using many randomized decision trees, but in turn introduce (lesser) inductive biases associated with the way an RF's underlying decision trees are contructed. One mitigant is to ensemble different learners, using a soft-max function or a voting approach. For voting based ensembling we would simply run a number of classifiers, take the majority answer: good/bad credit. (Note that we have included \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xjkMTtAW22C6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "xjkMTtAW22C6",
    "outputId": "670b0921-097f-4e70-b3a2-72b8c4b54818"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run the challenger ensemble\n",
    "y_test_hat, ens_mdl = challenger_ensemble_run(all_mdls=all_models,\n",
    "                        all_mdls_desc=all_models_desc,\n",
    "                        all_mdls_prec=all_mdls_prec,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test)\n",
    "\n",
    "# Analyst KPI...\n",
    "f1, prec, rec = kpi_review_analyst(mdl=ens_mdl,X=X_test.values, y=y_test[:].values, y_hat=y_test_hat[:])\n",
    "print(f1, prec, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wfSaSlGfw66d",
   "metadata": {
    "id": "wfSaSlGfw66d"
   },
   "source": [
    "### DRAFT!!!! FAIRNESS FUNCTIONS Removing features correlated with protected characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cVV8YHYuwfRU",
   "metadata": {
    "id": "cVV8YHYuwfRU"
   },
   "source": [
    "Let us inspect the results between the \"fair\" model outcomes and the potentially biased...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LvIWjJXQtAVd",
   "metadata": {
    "id": "LvIWjJXQtAVd"
   },
   "source": [
    "## Stage 3c: Fairness\n",
    "\n",
    "We need to ensure that accuracy is similar across different population groups, in other words that there is no unfairness in model outcomes that can be linked to these groups. To do this we use the utilities weintroduced earlier in the notebook.\n",
    "\n",
    "We need to examine recall to ensure fairness (a customer KPI) and precision (a business KPI) to ensure we have an acceptable false positive rate. For the fairness tests we will look at different population groups: gender(Male/Female) and Age Group (Below 25/25-60/Above 60 age groups. In a fair model, each of the population groups should not be more than 20% worse-off than the majority group. \n",
    "The plots display the range as between the two red lines. Any population group with a metric value outside the range between the red lines is impacted negatively by the model. Adjusting the model parameters or\n",
    "\n",
    "Higher the recall values, the lower is the False Negatives. Higher the precision values, the lower is False Positives. A judgement call is neeeded to confirm if the false positives have more adverse impact or false negatives. In case of credit approval process, for a loan applicant, false negatives should be minimized. So recall is the more appropriate measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SXogafeXHFQS",
   "metadata": {
    "id": "SXogafeXHFQS"
   },
   "source": [
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "#Madhu... Fairness applied to the chosen model....\n",
    "\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2872b8b-5ab3-427a-865c-682b1c325de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added by Madhu\n",
    "X_test_gender = X_test_protected['gender']  #@@@@@@@@@@@@@@@@@@@\n",
    "df_stats = fairness_stats_get (max_mdl, X_test, y_test, X_test_gender)\n",
    "\n",
    "plot_fairness_charts(df_stats, \"Male\", [\"recall\", \"precision\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a655f46c-0b6d-4dbe-afec-c2ad95f9bacf",
   "metadata": {},
   "source": [
    "The machine learning models return a probability that the outcome is one of approve/reject.  the use case would use a specific pobability threshold to convert the outcome to a tangible decision. By default the models use a probability threshold of 0.5. If the probability of an outcome exceeds the threshold of 0.5, then xxxx.\n",
    "\n",
    "The lender might choose to utilize a higher than 50% probability to narrow down on a specific outcome.Selecting the apprpriate threshold has a big impact on the model metrics as well as fairness outcomes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da6d6f-67fa-4d06-87f0-f0344c9ad973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added by Madhu\n",
    "df_stats = fairness_stats_get (max_mod, X_test, y_test, X_test_gender, 0.5)\n",
    "\n",
    "plot_fairness_charts(df_stats, \"Male\", [\"recall\", \"precision\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86906860-0692-4e78-b95b-31df6303a846",
   "metadata": {},
   "source": [
    "In the above example, it is seen that using a threshold of 0.5, the model is not seen fair - the recall value Females is outside of the 20% rane from the recall value for Males, indicating that the decision makes females worse off compared to Males. Now let us try to run the model with a threshold of 0.9, i.e. the model outcome should have a probabiity exceeding 0.9 to accept a specific response.  In this case, it is seen that the model is fair and has a high recall, while the precision is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4cec2-4bff-451e-b0be-5f58cde4c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added by Madhu\n",
    "df_stats = fairness_stats_get (max_mod, X_test, y_test, X_test_gender, 0.9)\n",
    "\n",
    "plot_fairness_charts(df_stats, \"Male\", [\"recall\", \"precision\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b53b93-5f84-42c7-acf2-98176eece651",
   "metadata": {},
   "source": [
    "#Added by Madhu\n",
    "\n",
    "Utilize the process process defined earlier to identify an appropriate model in this context by reviewing recall values that are within acceptable limits for different population groups, concurrently maximizing the overall model precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23387074-a98c-4471-963b-fb9186123c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added ny Madhu\n",
    "#get the optimal threhold to use, by iterating with different values of threshold, that determines the model outcomes.\n",
    "\n",
    "optimal_threshold = get_optimal_threshold (max_mod, X_test, y_test, X_test_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0LFyHIAP59O7",
   "metadata": {
    "id": "0LFyHIAP59O7"
   },
   "source": [
    "# Stage4: Model Deployment \n",
    "\n",
    "Model deployment generally involves a change process, several levels of testing and sign off, asignment of responsibilities for the live operation of the process, models and data before deployiong the code to the cloud (or on native hardware). \n",
    "A key part of thius stage is communication of the KPIs to stakleholders to enable them to understand the way the models operate, the risks involved and to be accountable for deploying the models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xDmGSQUV2Fle",
   "metadata": {
    "id": "xDmGSQUV2Fle"
   },
   "source": [
    "## Stage4a. KPIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JZGI4nBO0HlI",
   "metadata": {
    "id": "JZGI4nBO0HlI"
   },
   "source": [
    "## Stage4b. Communication: Stakeholder Oriented Explanations\n",
    "\n",
    "Communication of how the model has reached the outcomes it is has, is crutial to achieve fairness, transparency, accountability, and trust in the whole process. Each stakeholder in the process needs to see different elements. The Data Scientist and technical leadership need to review the nut and bolts of the model, reviewing residuals plots, parameter importance, interaction terms and many other metrics. The customer needs to see far less information, and mainly that associated with a refusal of credit. Compliance resources and regulators need to see something different again, such as fairness regarding protected characteristics, the accuracy and therefore capital risk represented by the models. \n",
    "In this section we look at stakeholder oriented explanations and we will be using standard charts of important analytics, such as residual plots, and SHAP.\n",
    "\n",
    "First let us get the packages we will need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dWv02uvQWyd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dWv02uvQWyd",
    "outputId": "3910f0ab-05af-4018-9183-e608cada0582"
   },
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nx16rPvNQdrZ",
   "metadata": {
    "id": "Nx16rPvNQdrZ"
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ku_I-kcG8bpR",
   "metadata": {
    "id": "Ku_I-kcG8bpR"
   },
   "source": [
    "Now instantiate the SHAP explainer object for our classifier and generate Shapley values for the test data... Note that as we will be using the test data, all the analysis is therefore based on the out of sample performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_xCoD7kw6r0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xCoD7kw6r0b",
    "outputId": "80680d90-301e-4f8b-d429-1f259e291a5e"
   },
   "outputs": [],
   "source": [
    "print(type(max_mdl).__name__)\n",
    "\n",
    "# Instantiate an explainer object for our chosen classifier...\n",
    "if type(max_mdl).__name__ == 'DecisionTreeClassifier':\n",
    "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'GaussianNB':\n",
    "  explainer = shap.KernelExplainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer.shap_values(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'LogisticRegression':\n",
    "  explainer = shap.explainers.Permutation(max_mdl.predict, X_test)\n",
    "  shap_values = explainer(X_test)\n",
    "elif type(max_mdl).__name__ == 'MLPClassifier':\n",
    "  explainer = shap.KernelExplainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer.shap_values(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'RandomForestClassifier':\n",
    "  explainer = shap.Explainer(max_mdl.predict, X_test)\n",
    "  shap_values = explainer(X_test)\n",
    "elif type(max_mdl).__name__ == 'LinearDiscriminantAnalysis':\n",
    "  masker = shap.maskers.Independent(data = X_test.values)\n",
    "  explainer = shap.LinearExplainer(max_mdl, masker = masker)\n",
    "  shap_values = explainer(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'QuadraticDiscriminantAnalysis':\n",
    "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'AdaBoostClassifier':\n",
    "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'GradientBoostingClassifier':\n",
    "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'KNeighborsClassifier':\n",
    "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'SVC':\n",
    "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer(X_test.values)\n",
    "elif type(max_mdl).__name__ == 'NuSVC':\n",
    "  explainer = shap.Explainer(max_mdl.predict, X_test.values)\n",
    "  shap_values = explainer(X_test.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdMhCBHMFf4v",
   "metadata": {
    "id": "bdMhCBHMFf4v"
   },
   "source": [
    "### 1) Analyst and technical explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fPlSWba2dSoo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "fPlSWba2dSoo",
    "outputId": "46f2e55f-0a29-4ed8-b050-2940ac1981bf"
   },
   "outputs": [],
   "source": [
    "# Analyst KPI...\n",
    "kpi_review_analyst(mdl=max_mdl,X=X_test, y=y_test, y_hat=y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fQA6gVOre56h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "fQA6gVOre56h",
    "outputId": "de2ebfd9-16b4-4ef5-c669-3daccfef3678"
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8u7btmtyFKNn",
   "metadata": {
    "id": "8u7btmtyFKNn"
   },
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NfwlscWzExPQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "id": "NfwlscWzExPQ",
    "outputId": "a119aabf-c39c-4957-85d5-7ffce6fbb752"
   },
   "outputs": [],
   "source": [
    "# Plot the feature importance\n",
    "shap.plots.bar(shap_values=shap_values, max_display=30, show=False)\n",
    "plt.title(\"Feature Importance: Credit-Use Case Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0S8f3dp_xkqS",
   "metadata": {
    "id": "0S8f3dp_xkqS"
   },
   "source": [
    "Beeswarm plot, showing features ranking by importance, and the impact of each instance passed to the shap explainer. Note the feature value is color coded from \"high\" to \"low\" (vertical axis), and the Shap value represents the impact on the model output of the value (horixontal axis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r0GjSB9YxlEl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "id": "r0GjSB9YxlEl",
    "outputId": "9cebf4a2-24a6-4f2f-d35d-47447533eaaf"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, show=False)\n",
    "plt.title(\"Beeswarm: Credit-Use Case Feature Importance and Dependency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fcWInh48Q",
   "metadata": {
    "id": "ce1fcWInh48Q"
   },
   "source": [
    "### 2) Compliance and Regulatory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxV86fMFKX7r",
   "metadata": {
    "id": "fxV86fMFKX7r"
   },
   "source": [
    "Qualitatively check to see if any features are suspiciously important for one (eg) gender vs another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nqJGtpmu1nig",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nqJGtpmu1nig",
    "outputId": "2155ad9d-57d1-45fd-fd24-62441382e825"
   },
   "outputs": [],
   "source": [
    "# Feature importance by protected characteristic.. different treatment?\n",
    "for prot_char in X_test_protected:\n",
    "  # Extract the protected classes.\n",
    "  curr_prot_cats = X_test_protected[prot_char].astype(str).to_list()\n",
    "\n",
    "  # Plot the feature importance\n",
    "  shap.plots.bar(shap_values.cohorts(curr_prot_cats).abs.mean(0), show=False)\n",
    "  plt.title(\"Bias Check: Feature Importance of protected group: \" + prot_char)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5MGWlux3KvFy",
   "metadata": {
    "id": "5MGWlux3KvFy"
   },
   "source": [
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "Madhu\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "De8c3JUdKvtR",
   "metadata": {
    "id": "De8c3JUdKvtR"
   },
   "source": [
    "### 3) Customer\n",
    "\n",
    "A customer may want to knowlt he sensitivity of the decision to certain characteristics. IF a refusal has been made, it would be beneficial to report to the customer what they need to change to get a favorable outcome. \n",
    "\n",
    "We can use the SHAP waterfall plot to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ZHyTdYTLCh8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "4ZHyTdYTLCh8",
    "outputId": "e7f367c7-927d-4f42-8026-535dddbeaa2a"
   },
   "outputs": [],
   "source": [
    "# Find a customer with bad credit prediction\n",
    "for rejected_eg_rowno in range(0,y_test.shape[0]):\n",
    "  if y_test.iloc[rejected_eg_rowno] == 1:\n",
    "    break\n",
    "\n",
    "# This customer was refused credit and we can provide an explanation for their refusal...\n",
    "shap.plots.waterfall(shap_values[rejected_eg_rowno])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dha9E2RRLRvI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "dha9E2RRLRvI",
    "outputId": "e38b960a-219f-4628-b570-45fe9e1b1e43"
   },
   "outputs": [],
   "source": [
    "y_test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TTN8r_7zRDbt",
   "metadata": {
    "id": "TTN8r_7zRDbt"
   },
   "source": [
    "# Stage 5: Model Monitoring and Reporting\n",
    "\n",
    "Our model is now in production and being used in practice. We are \"risk on\" and we need to continually monitor and record our KPIs. Individuals accountability for these processes is critical.\n",
    "\n",
    "We also need to monitor data drift. If the distribution or the nature of the data we pass into our model substantioally differs from our training data, our model results will almost certainly be garbage. We need to monitor data drift and should drift occurr we need to re-run our model developmnent process to traing an appropriate model. This would take us back to Stage2 in this process.\n",
    "\n",
    "There is also the question of whether our model is still the best approach as time steps forward? Our model paramaters may become stale, the model itself may be less appropriaytr given its inductive biases. Many things can change and to monitor this we should consider using a challenger model. Challenger models are used to compete against our live model, and we should monitor our KPIs generated by our live model and compare them to those produced by the challenger model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3se-Ud-Spu3",
   "metadata": {
    "id": "j3se-Ud-Spu3"
   },
   "source": [
    "## Stage5a: Data Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QREnQ1poSmRR",
   "metadata": {
    "id": "QREnQ1poSmRR"
   },
   "outputs": [],
   "source": [
    "# AUGUSTINE! Add your function call HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4wmYyfyWRmrR",
   "metadata": {
    "id": "4wmYyfyWRmrR"
   },
   "source": [
    "## Stage5b: Challenger Models\n",
    "\n",
    "So what model do we use as a challenger, and when do we traing it? There are no perfect answers but we can use our model development pipeline to help. Note that our model selection function, auto_classifier_selection, allowed us to generate a range of classifiers, we can use one or all of these classifiers as our challenger. If we decided to ensemble the classifiers as our live model (ir combine the results of all in a voting ensemble for instance) we can select the best single model as our challeger. If we decided to use the best single model as our live model, we can use a boting ensemble as our challenger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_8v9zlaWTd3F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "_8v9zlaWTd3F",
    "outputId": "ba41cfb2-8916-4500-c9c8-673eb5ccf85f"
   },
   "outputs": [],
   "source": [
    "# Run the challenger ensemble\n",
    "y_test_hat, ens_mdl = challenger_ensemble_run(all_mdls=all_models,\n",
    "                        all_mdls_desc=all_models_desc,\n",
    "                        all_mdls_prec=all_mdls_prec,\n",
    "                        X_train=X_train,\n",
    "                        y_train=y_train,\n",
    "                        X_test=X_test)\n",
    "\n",
    "# Do we need to retrain the live model?\n",
    "kpi_review_challenger(live_mod=max_mdl, challenger_mod=ens_mdl, X_test=X_test, y_test=y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
